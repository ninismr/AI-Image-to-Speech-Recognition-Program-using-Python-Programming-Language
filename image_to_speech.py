# -*- coding: utf-8 -*-
"""Image-to-Speech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PKXdMy6QQU8QBBTXwvRjj3W5MCriZu7i
"""

#install and import opencv library to do denoising for the image
!pip install opencv-python-headless==4.1.2.30
import cv2

#install and import easyocr library to read text from image
!pip install easyocr
import easyocr

#install and import googletrans library to translate the text from image
!pip install install googletrans==3.1.0a0
from googletrans import Translator

#install and import gTTS library to convert text-to-speech
!pip install gTTS
from gtts import gTTS

#import Ipython.display.audio to directly play the audio in the python notebook
from IPython.display import Audio

#select the language to extract the text
#for the below example we want to extract it from the image that contains the Japanese language 
reader = easyocr.Reader(['ja'])

#Translator class used to translate text from one language into another language
translator = Translator()

#import PIL (Python Imaging Library) which provides the python interpreter with image editing capabilities
import PIL

#import ImageDraw module from PIL to draw the bounding boxes in the given image
from PIL import ImageDraw

#open and read the image that wants to extract the text from
im = PIL.Image.open('story(jpn).png')
im

#import matplotlib library to show the denoising images result
from matplotlib import pyplot as plt

#do denoising images to improve the OCR accuracy
img = cv2.imread('story(jpn).png')
dst = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)
plt.figure(figsize=(50,50))
plt.subplot(121),plt.imshow(img)
plt.subplot(122),plt.imshow(dst)
plt.show()

#draw the bounding box for the text in the image (text detection)
def draw_boxes(image, color='red', width=2):
    draw = ImageDraw.Draw(image)
    for bound in bounds:
        p0, p1, p2, p3 = bound[0]
        draw.line([*p0, *p1, *p2, *p3, *p0], fill=color, width=width)
    return image

draw_boxes(im)

#get the text in the bounding box in the image in a detailed output (text recognition)
#the output will be in a list format, each item represents lists of bounding box coordinates [x,y], the text detected and model confident level, 
#respectively
#image (string, numpy array, byte) - Input image
#add_margin (float, default = 0.1) - Extend bounding boxes in all direction by certain value. This is important for language with complex script
#width_ths (float, default = 0.5) - Maximum horizontal distance to merge boxes
#link_threshold (float, default = 0.4) - Link confidence threshold
#decoder (string, default = 'greedy') - options are 'greedy', 'beamsearch' and 'wordbeamsearch'.
#blocklist (string) - Block subset of character. This argument will be ignored if allowlist is given
bounds = reader.readtext(dst, add_margin=0.55, width_ths=0.7, link_threshold=0.8, decoder='beamsearch', blocklist='=-')
bounds

#get the text in the bounding box in the image in a simple output by using detail=0 parameter (text recognition)
#the output will be in a list format but in a simple way, just the text detected, without a bounding box coordinates [x,y] and model confident level
#image (string, numpy array, byte) - Input image
#add_margin (float, default = 0.1) - Extend bounding boxes in all direction by certain value. This is important for language with complex script
#width_ths (float, default = 0.5) - Maximum horizontal distance to merge boxes
#link_threshold (float, default = 0.4) - Link confidence threshold
#decoder (string, default = 'greedy') - options are 'greedy', 'beamsearch' and 'wordbeamsearch'.
#blocklist (string) - Block subset of character. This argument will be ignored if allowlist is given
#detail (int, default = 1) - Set this to 0 for simple output
text_list = reader.readtext(dst, add_margin=0.55, width_ths=0.7, link_threshold=0.8, decoder='beamsearch', blocklist='=-', detail=0)
text_list

#combine/join the detected text in the image from multiple lines into the single-line text
text_comb=' '.join(text_list)
text_comb

#detect the language of the text in the image using translator
print(translator.detect(text_comb))

#translate the detected language of the text into a default language (English)
text_en = translator.translate(text_comb)
print(text_en.text)

#read the text audio in a default accent (English)
ta_tts = gTTS(text_en.text)

#save the text audio file
ta_tts.save('trans_en.mp3')

#play the text audio file in an English accent
Audio('trans_en.mp3', autoplay=True)

#translate the detected language of the text into the Indonesian language
text_id = translator.translate(text_comb, src='ja', dest='id')
print(text_id.text)

#read the text audio in an Indonesian accent 
ta_tts_id = gTTS(text_id.text, lang='id')

#save the text audio file
ta_tts_id.save('trans_id.mp3')

#play the text audio file in an Indonesian accent
Audio('trans_id.mp3', autoplay=True)

#translate the detected language of the text into the Japanese language
text_ja = translator.translate(text_comb, src='ja', dest='ja')
print(text_ja.text)

#read the text audio in a Japanese accent 
ta_tts_ja = gTTS(text_ja.text, lang='ja')

#save the text audio file
ta_tts_ja.save('trans_ja.mp3')

#play the text audio file in a Japanese accent
Audio('trans_ja.mp3', autoplay=True)